# LLM Discussion Summarization - Setup & Testing Guide

## Overview
This guide walks through setting up and testing the LLM-based discussion summarization feature that automatically detects gamedev discussions, performs relevance checks, and generates comprehensive summaries with admin approval.

## Table of Contents
1. [Environment Setup](#environment-setup)
2. [Configuration](#configuration)
3. [Manual Testing](#manual-testing)
4. [Troubleshooting](#troubleshooting)
5. [Monitoring & Maintenance](#monitoring--maintenance)

---

## Environment Setup

### 1. Install Dependencies
```bash
npm install
```

This will install `@google/generative-ai` and all other required packages.

### 2. Obtain API Keys

#### Gemini API Key (Required)
1. Visit [Google AI Studio](https://aistudio.google.com/apikey)
2. Click "Create API Key"
3. Create a new API key in default location
4. Copy the key to `.env` as `GEMINI_API_KEY`

### 3. Environment Variables

Add these to your `.env` file:

```bash
# Gemini LLM API Key (required)
GEMINI_API_KEY=your_gemini_api_key_here

# Feature toggles
LLM_SUMMARY_ENABLED=true
LLM_DRY_RUN=false              # Set to true to count tokens without API cost
LLM_PROVIDER=gemini

# Discord Channel IDs
LLM_ADMIN_APPROVAL_CHANNEL=1234567890  # Channel where admins review summaries
LLM_SUMMARY_CHANNEL=0987654321         # Channel where summaries are posted

# Optional: Channel whitelist (comma-separated, empty = all channels)
LLM_CHANNEL_WHITELIST=channel_id_1,channel_id_2

# LLM Settings
LLM_MIN_MESSAGES=10           # Minimum messages before triggering LLM check
LLM_LOOKBACK_WINDOW=100       # How many messages to look back
```

### 4. Verify Configuration

Run this to verify setup:

```bash
npm run dev
```

You should see in logs:
```
[LLMService] Initialized with provider: gemini
[LLMSummaryManager] State loaded
```

---

## Configuration

### Channel Setup

#### Admin Approval Channel
- **Purpose**: Admins receive notifications here to review and approve summaries
- **Required Permissions**: Bot needs Send Messages, Embed Links, Add Reactions
- **Type**: Text channel (public or private)

#### Summary Channel
- **Purpose**: Final summaries are posted here
- **Required Permissions**: Bot needs Send Messages, Embed Links
- **Type**: Text channel (can be read-only)

### Rate Limits & Thresholds

All thresholds can be configured in `config/config.js` under `LLM_SUMMARY`:

```javascript
filters: {
  minMessages: 10,              // Min messages to trigger check
  lookbackWindow: 100,          // Max messages to analyze
  relevanceThreshold: 0.7,      // 0-1, minimum confidence for "relevant"
},
rateLimits: {
  maxRequestsPerHour: 10,       // Global hourly limit
  cooldownBetweenChecks: 30 * 60 * 1000,  // 30 min per channel
}
```

### Active Chat Detection Integration

The LLM Summary feature is automatically triggered when `activeChatManager` detects a hot channel. Existing rules:
- **Rule 1**: 3+ users, 10+ messages in 60 minutes
- **Rule 2**: 4+ users, 8+ messages in 45 minutes

---

## Manual Testing

### Test Scenario 1: Basic Relevance Check

**Goal**: Verify that the bot correctly identifies gamedev-related discussions

**Steps**:
1. In a channel listed in `LLM_CHANNEL_WHITELIST` (or any if whitelist is empty):
2. Have 3+ users post 10+ messages within 60 minutes about game development
   - Example: "Unity好用嗎？", "Unreal的渲染管道很強", "但Unity更輕量..."
3. Trigger active chat detection (reach message/user thresholds)
4. Check the admin approval channel for notification embed

**Expected Result**:
- ✅ Admin receives notification with:
  - Green/blue color for "technics" category
  - 相關度 (confidence) > 70%
  - "生成完整摘要" button enabled
  - Message preview showing part of conversation

---

### Test Scenario 2: Admin Approval Workflow

**Goal**: Verify admin can approve and generate full summary

**Steps**:
1. Complete Test Scenario 1
2. In admin approval channel, click "生成完整摘要" button
3. Wait for bot response (should be ~3-5 seconds)
4. Check the summary channel for final output

**Expected Result**:
- ✅ Button shows loading state ("⏳ 正在生成完整摘要...")
- ✅ Bot responds with "✅ 摘要已生成並發佈到摘要頻道"
- ✅ Summary appears in summary channel with:
  - Title (auto-generated by LLM)
  - Full summary (100-200 words)
  - Key points (3-5 bullet points)
  - Participants list
  - Resources/links if mentioned
  - Action items if specified

---

### Test Scenario 3: Rejection Handling

**Goal**: Verify admin can reject summaries

**Steps**:
1. Complete Test Scenario 1
2. In admin approval channel, click "忽略" button
3. Check admin approval channel for confirmation

**Expected Result**:
- ✅ Button shows confirmation message
- ✅ Summary ID is removed from pending list
- ✅ No summary posted to summary channel
- ✅ State cleaned up after 1 minute

---

### Test Scenario 4: Rate Limiting

**Goal**: Verify rate limiting prevents excessive API calls

**Steps**:
1. In 10 different channels, trigger active chat detection (reach thresholds)
   - This will attempt 10 LLM calls
2. Try to trigger detection in an 11th channel
3. Check bot logs

**Expected Result**:
- ✅ First 10 checks trigger normally
- ✅ 11th check is blocked with log: "Rate limit exceeded for channel"
- ✅ Resets after 1 hour

---

### Test Scenario 5: Channel Cooldown

**Goal**: Verify per-channel 30-minute cooldown

**Steps**:
1. In a single channel, trigger active chat detection (reach thresholds)
   - Admin receives notification
2. Immediately trigger the same detection again
   - Post another set of 10+ messages from 3+ users

**Expected Result**:
- ✅ First detection triggers → Admin notification sent
- ✅ Second detection is blocked with log: "Rate limit exceeded for channel"
- ✅ Can trigger again after 30 minutes

---

### Test Scenario 6: Non-Relevant Discussion

**Goal**: Verify irrelevant discussions are skipped

**Steps**:
1. In a channel, have 3+ users post 10+ messages about non-gamedev topics
   - Example: "大家晚餐吃什麼？", "我喜歡看動畫", "今天天氣真好"
2. Trigger active chat detection

**Expected Result**:
- ✅ No notification in admin channel
- ✅ Bot logs: "Not relevant (confidence: 0.X)"
- ✅ No state saved

---

### Test Scenario 7: Error Handling - Invalid API Key

**Goal**: Verify graceful degradation on API errors

**Steps**:
1. Set `GEMINI_API_KEY=invalid_key` in `.env`
2. Restart bot: `npm run dev`
3. Trigger active chat detection

**Expected Result**:
- ✅ No crash or hang
- ✅ Bot logs error: "[GeminiProvider] Error: ..."
- ✅ Admin is notified of error if possible

---

### Test Scenario 8: Error Handling - Timeout

**Goal**: Verify timeout handling

**Steps**:
1. Temporarily modify `llmService.js` to add delays
2. Trigger active chat detection
3. Wait for 30+ seconds

**Expected Result**:
- ✅ No hang - operation times out after 30s
- ✅ Bot logs: "[LLMService] ... timeout"
- ✅ Graceful error response

---

### Test Scenario 9: State Persistence

**Goal**: Verify pending summaries survive bot restart

**Steps**:
1. Complete Test Scenario 1 (get notification)
2. **Don't** click approve/reject yet
3. Restart bot: `npm run dev`
4. Check admin approval channel - notification should still be there
5. Click "生成完整摘要"

**Expected Result**:
- ✅ Pending summary is loaded from state after restart
- ✅ Summary can still be approved
- ✅ Complete workflow works

---

### Test Scenario 10: Message Cleanup

**Goal**: Verify expired summaries are cleaned up

**Steps**:
1. Complete Test Scenario 1
2. Set `adminApprovalTimeout` to 10 seconds (in code, for testing)
3. Wait 12 seconds
4. Restart bot
5. Check bot logs

**Expected Result**:
- ✅ Expired summary is removed during cleanup
- ✅ Bot logs cleanup activity
- ✅ No stale data in state file

---

## Cost Estimation & Dry Run

### Dry Run Mode
To test the entire workflow (from detection to admin approval) without incurring any Gemini API costs, you can enable Dry Run mode.

**Setup**:
```bash
# In .env
LLM_DRY_RUN=true
```

**How it works**:
1. **Free Token Counting**: Uses the Gemini `countTokens` API (which is free) to calculate exactly how many tokens the prompt consumes.
2. **Cost Logging**: Logs the token count and an estimated USD cost to the console:
   `[LLM Token Cost] Relevance Check: 1234 tokens | Est. Cost: $0.000432 (Flash)`
3. **Mock Responses**: Instead of calling the LLM for generation, the bot returns high-quality mock data.
   - **Relevance Check**: Always returns `isRelevant: true` to allow testing the next steps.
   - **Summary**: Returns a "Dry Run Summary" message.

### Estimating Costs
Even with `LLM_DRY_RUN=false`, the bot will always log the token usage and estimated cost for every API call. This helps you monitor actual usage in production.

**Pricing Assumptions (Gemini 1.5 Flash)**:
- ~$0.35 per 1 million input tokens.
- ~$1.05 per 1 million output tokens.
- (Prices are estimated and may vary by region/tier).

---

## Troubleshooting

### Issue: "LLM provider not initialized"
**Cause**: API key missing or feature disabled
**Solution**:
1. Verify `GEMINI_API_KEY` is set in `.env`
2. Verify `LLM_SUMMARY_ENABLED=true` in `.env`
3. Restart bot

### Issue: Admin receives no notification
**Cause**: Channel not meeting active chat thresholds or LLM check failed
**Solution**:
1. Check logs: `[LLMSummaryManager] Processing hot channel`
2. Verify channel meets minimum message/user requirements
3. Check `LLM_ADMIN_APPROVAL_CHANNEL` ID is correct
4. Verify bot has permissions in that channel

### Issue: "生成完整摘要" button shows error
**Cause**: LLM API error or network issue
**Solution**:
1. Check bot logs for API error details
2. Verify internet connection
3. Check Gemini API quota/rate limits at [Google AI Studio](https://aistudio.google.com/apikey)
4. Try again in a few minutes

### Issue: Summary contains gibberish or is incomplete
**Cause**: LLM parsing error or token limit exceeded
**Solution**:
1. Reduce `LLM_LOOKBACK_WINDOW` to analyze fewer messages
2. Check bot logs for parsing errors
3. Verify prompt files are readable: `config/prompts/relevanceCheck.txt` and `comprehensiveSummary.txt`

### Issue: No state being saved
**Cause**: Disk write permissions or JSON corruption
**Solution**:
1. Verify `data/llmSummaryState.json` is readable/writable
2. Check file permissions: `ls -la data/llmSummaryState.json`
3. Check for JSON syntax errors in state file
4. Clear state: `echo '{}' > data/llmSummaryState.json` (CAUTION: loses pending summaries)

---

## Monitoring & Maintenance

### Logging

The system uses console logging with prefixes:
- `[LLMService]` - LLM API layer
- `[LLMSummaryManager]` - Main orchestration
- `[ConversationCollector]` - Message collection
- `[GeminiProvider]` - Gemini API calls
- `[SummaryApprove/Reject]` - Button interactions

### Cost Monitoring

**Estimate**: ~$0.01 per summary generated
- Relevance check: $0.0005 (Gemini Flash)
- Full summary: $0.01 (Gemini Pro)

Track usage in [Google AI Studio](https://aistudio.google.com/apikey)

### Daily Maintenance

- Monitor state file size: `ls -lh data/llmSummaryState.json`
- Check for expired summaries being cleaned up
- Review bot logs for errors or rate limit issues

### Weekly Maintenance

- Check API usage and costs
- Review summary quality in summary channel
- Adjust relevance threshold if too many false positives/negatives
- Check rate limit settings match usage patterns

---

## Advanced Configuration

### Custom Category Colors

Edit `llmSummaryManager.js` `CATEGORY_COLORS` object to customize embed colors:

```javascript
const CATEGORY_COLORS = {
    technics: 0x3498DB,   // Blue
    art: 0xE74C3C,        // Red
    design: 0x9B59B6,     // Purple
    news: 0xF39C12,       // Orange
    resource: 0x27AE60,   // Green
    other: 0x95A5A6       // Gray
};
```

### Custom Prompts

Edit prompt files to customize LLM behavior:
- `config/prompts/relevanceCheck.txt` - Adjust relevance criteria
- `config/prompts/comprehensiveSummary.txt` - Adjust summary format

### Rate Limit Tuning

For more aggressive summarization:
```javascript
rateLimits: {
  maxRequestsPerHour: 20,              // Increase limit
  cooldownBetweenChecks: 15 * 60 * 1000,  // Reduce cooldown
}
```

---

## Performance Optimization

### If LLM calls are slow:
1. Reduce `LLM_LOOKBACK_WINDOW` (fewer messages to analyze)
2. Use `gemini-1.5-flash` for both checks (trade quality for speed)
3. Increase `cooldownBetweenChecks` to reduce frequency

### If state file grows too large:
1. Reduce `adminApprovalTimeout` (clean up faster)
2. Check for orphaned summaries in `data/llmSummaryState.json`

---

## Rollback Plan

If issues arise:

1. **Disable feature**:
   ```bash
   # In .env
   LLM_SUMMARY_ENABLED=false
   ```

2. **Revert to last working version**:
   ```bash
   git checkout <commit-hash>
   npm install
   npm run dev
   ```

3. **Clear state if corrupted**:
   ```bash
   echo '{"pendingSummaries": {}, "rateLimits": {"hourlyRequests": {}, "channelCooldowns": {}}, "lastCleanup": 0}' > data/llmSummaryState.json
   ```

---

## Support & Debugging

### Enable Debug Logging

Uncomment debug lines in:
- `activeChatManager.js` line 115
- `conversationCollector.js` and `llmSummaryManager.js` for verbose logs

### Generate Debug Report

```bash
# Collect system info and logs
npm run test  # Run test suite to verify functionality
echo "API Key set: $([ -n "$GEMINI_API_KEY" ] && echo 'YES' || echo 'NO')"
echo "Feature enabled: $(grep LLM_SUMMARY_ENABLED .env)"
ls -la data/llmSummaryState.json
```

Share this report when seeking help.
